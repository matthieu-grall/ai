# Intelligence artificielle (IA)
## Critères de confiance

[Contributeurs](#contributeurs)<br/>
[Versions](#versions)<br/>
[Documents de référence](#documents-de-référence)<br/>
1. [Objet du document](#1-objet-du-document)
2. [Introduction](#2-introduction)
3. [Critères de confiance](#3-critères-de-confiance)

### Contributeurs
**Matthieu GRALL**, expert-conseil en management des données, sécurité de l’information, protection de la vie privée et nouvelles technologies.

<autres contributeurs>

### Versions
| <center>**Version**</center> | <center>**Action**</center> | <center>**Éditeur**</center> | <center>**État**</center> |
| --- | --- | --- | ---|
| 06/04/2025 (v0.1) | Création du document | Matthieu GRALL | Document de travail
| 10/04/2025 (v0.2) | Transformation des « descriptions » des critères de confiance en « Portées » (pour mieux distinguer « Portées », qui désignent ce qui est couvert, et « objectifs », qui sont présentés dans les bonnes pratiques), corrections mineures (mises en cohérence avec les autres documents) | Matthieu GRALL | Document de travail
| 23/04/2025 (v0.3) | Transformation en _markdown_ | Matthieu GRALL | Document de travail

### Documents de référence
| <center>**Libellé court**</center> | <center>**Libellé long**</center> |
| --- | --- |
| [EN 301 549] | Exigences d’accessibilité pour les produits et services ICT, _European Telecommunications Standards Institute_ (ETSI, 2018)<br/>[Lien](https://accessibilite.numerique.gouv.fr/doc/fr_301549v020102p.pdf) |
| [ISO/IEC 42001] | Technologies de l’information – Intelligence artificielle – Système de management, International _Organization for Standardization_ (ISO, 2023)<br/>[Lien](https://www.iso.org/fr/standard/81230.html) |
| [Loi I&L] | Loi n°78-17 du 6 janvier 1978 relative à l’informatique, aux fichiers et libertés, modifiée<br/>[Lien](https://www.legifrance.gouv.fr/loda/id/JORFTEXT000000886460/) |
| [NIST AI RMF] | Artificial Intelligence Risk Management Framework (AI RMF 1.0), NIST AI 100-1, _National Institute of Standards and Technology_ (NIST, 2024)<br/>[Lien](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf) |
| [Principes de l'OCDE sur l'IA] | Principes de l’OCDE sur l’IA (2019)<br/>[Lien](https://www.oecd.org/fr/topics/sub-issues/ai-principles.html) |
| [Recommandation IA de l'OCDE] | Recommandation du Conseil sur l’intelligence artificielle, OECD/LEGAL/0449, Organisation de coopération et de développement économiques (OCDE, 2023)<br/>[Lien](https://legalinstruments.oecd.org/fr/instruments/oecd-legal-0449) |
| [Recommandations IA de l'ANSSI - 2024] | Recommandations de sécurité pour un système d’IA générative, Agence nationale de la sécurité des systèmes d’information (ANSSI, 2024)<br/>[Lien](https://cyber.gouv.fr/sites/default/files/document/Recommandations_de_s%C3%A9curit%C3%A9_pour_un_syst%C3%A8me_d_IA_g%C3%A9n%C3%A9rative.pdf) |
| [Recommandations IA de l'ANSSI - 2025] | Développer la confiance dans l’IA par une approche par les risques cyber (ANSSI, 2025)<br/>[Lien](https://cyber.gouv.fr/sites/default/files/document/analyse_commune_haut_niveau_des_risques_cyber_ia.pdf) |
| [Recommandations IA de la CNIL] | Recommandations sur le développement des systèmes d’intelligence artificielle, Commission nationale de l’informatique et des libertés (CNIL, 2024)<br/>[Lien](https://www.cnil.fr/fr/les-fiches-pratiques-ia) |
| [Règlement IA] | Règlement (UE) 2024/1689 du Parlement européen et du Conseil du 13 juin 2024 établissant des règles harmonisées concernant l’intelligence artificielle […] (règlement sur l’intelligence artificielle, 2024)<br/>[Lien](https://eur-lex.europa.eu/legal-content/FR/TXT/?uri=CELEX:32024R1689) |
| [RGAA] | Référentiel général d’amélioration de l’accessibilité (RGAA), Direction interministérielle du numérique (DINUM, 2023)<br/>[Lien](https://accessibilite.numerique.gouv.fr/doc/RGAA-v4.1.2.pdf) |
| [RGESN] | Référentiel général d’écoconception de services numériques (RGESN), Autorité de régulation de la communication audiovisuelle et numérique (ARCOM, 2024)<br/>[Lien](https://ecoresponsable.numerique.gouv.fr/docs/2024/rgesn-mai2024/referentiel_general_ecoconception_des_services_numeriques_version_2024.pdf) |
| [RGI] | Référentiel général d’interopérabilité (RGI), Direction interministérielle du numérique (DINUM, 2020)<br/>[Lien](https://www.numerique.gouv.fr/uploads/Referentiel_General_Interoperabilite_V2.pdf) |
| [RGIAF] | Référentiel général pour l’IA frugale (RGIAF), ministère de la transition écologique et de la cohésion des territoires (2024)<br/>[Lien](https://greentechinnovation.fr/storage/2024/06/Referentiel-general-pour-lIA-frugale.pdf) |

### 1. Objet du document
**Ce document propose une liste de critères de confiance, qui harmonise les différents critères des nombreux documents de références en matière d’intelligence artificielle (IA)**.

Il s’inscrit dans un ensemble de documents méthodologiques en amélioration continue, destinés à aider les organismes à gérer les risques liés à l’IA, et qui peuvent être utiles ensemble ou séparément :
1. [Exemples de micro-cas d'usages de l’IA](https://github.com/matthieu-grall/ai/blob/main/IA%20-%20Gestion%20des%20risques%20-%20Micro-cas%20d'usages%20-%20Exemples.md) ;
2. [Critères de confiance de l’IA](https://github.com/matthieu-grall/ai/blob/main/IA%20-%20Gestion%20des%20risques%20-%20Crit%C3%A8res%20de%20confiance.md) ;
3. [Bonnes pratiques de l'IA](https://github.com/matthieu-grall/ai/blob/main/IA%20-%20Gestion%20des%20risques%20-%20Bonnes%20pratiques.md) ;
4. [Méthode de gestion des risques de l’IA](https://github.com/matthieu-grall/ai/blob/main/IA%20-%20Gestion%20des%20risques%20-%20M%C3%A9thode.md).

Il sert de base à la constitution de bonnes pratiques harmonisées, et pourrait également servir à affiner les critères utilisés dans une démarche de gestion des risques.

### 2. Introduction
**Pour obtenir une liste de critères de confiance des systèmes basés sur l’IA, on se heurte à la pluralité des principaux documents de référence**<sup><a href="#note1" id="ref1">[1]</a></sup> qui comprennent des exigences, règles et recommandations liées à l’IA ou applicables à l’IA. Ils ne sont pas vraiment cohérents, que ce soit en termes de champs d’application, de formulations, de classements, et de langue. On a donc de nombreuses redondances dans l’ensemble et de nombreux manques dans chaque, si on souhaite une vision globale.

**Toutefois, les idées convergent toutes, ou se complètent plutôt bien**. On peut donc faire émerger une liste de ces critères de confiance, exhaustive et « non recouvrante », qui traite de l’ensemble des aspects qui peuvent devoir être considérés quand on crée un système basé sur l’IA.

Ainsi, tout système basé sur l’IA devrait :
1. reposer sur une **gouvernance responsable** (notion d’_accountability_) ;
2. assurer la **fiabilité**, et la **sûreté** des personnes et des biens ;
3. être *équitable* (notion de _fairness_) ;
4. garantir la **transparence** ;
5. assurer la **sécurité des informations** ;
6. protéger les **droits et libertés des personnes** ;
7. assurer la **maintenabilité** et l’**évolutivité** ;
8. permettre l’**interopérabilité** ;
9. respecter l’**environnement** ;
10. assurer l’**accessibilité**.

### 3. Critères de confiance
#### 3.1. Gouvernance responsable
Portée : pilotage éthique, transparent et responsable, incluant la mise en place de mécanismes de contrôle, de _reporting_ et de gestion des conflits d’intérêts.
<br/>
<br/>
Principaux risques : décisions unilatérales, conflits d’intérêts, manque de supervision, opacité décisionnelle.
#### 3.2. Fiabilité et sûreté
Portée : robustesse, performance, stabilité, résilience, précision, absence d'erreurs, capacité à fonctionner correctement, sans mettre en danger la vie humaine, les biens ou l'environnement, dans des conditions variées, incertaines ou inattendues, même en conditions de stress ou d’attaque.
<br/>
<br/>
Principaux risques : dysfonctionnements, accidents, attaques malveillantes, défaillances techniques.
#### 3.3. Équité
Portée : fonctionnement de l’IA sans biais, traitement impartial et équitable des usagers, non-discrimination.
<br/>
<br/>
Principaux risques : discriminations ou inégalités d’accès, du fait de biais dans la formulation des cas d’usages<sup><a href="#note2" id="ref2">[2]</a></sup>, liés aux données d’entrée<sup><a href="#note3" id="ref3">[3]</a></sup>, aux données d’entrainement et de validation<sup><a href="#note4" id="ref4">[4]</a></sup>, à l’algorithme d’entrainement / à l’architecture du modèle<sup><a href="#note5" id="ref5">[5]</a></sup>, aux données de sortie<sup><a href="#note6" id="ref6">[6]</a></sup>.
#### 3.4. Transparence
Portée : compréhension des processus et décisions, explicabilité, traçabilité des données, possibilité de contester et vérifier le fonctionnement.
<br/>
<br/>
Principaux risques : opacité, incompréhension des mécanismes, perte de confiance.
#### 3.5. Sécurité des informations
Portée : protection de la disponibilité, de l’intégrité et de la confidentialité des données, gestion des risques liés à la sécurité de l’information engendrés par les systèmes d’IA (au-delà du critère de confiance de fiabilité).
<br/>
<br/>
Principaux risques : cyberattaques, fuites de données, corruption de systèmes.
#### 3.6. Protection des droits et libertés
Portée : respect de la vie privée, protection des données à caractère personnel et des droits fondamentaux, gestion des risques sur les droits et libertés des personnes concernées engendrés par les systèmes d’IA (au-delà du critère de confiance d’équité).
<br/>
<br/>
Principaux risques : atteintes à la vie privée, usage abusif des données personnelles.
#### 3.7. Maintenance et évolutivité
Portée : maintien en conditions opérationnelle et de sécurité, adaptation du système d’IA au fil du temps pour résoudre des problèmes et être utilisé pour de nouveaux besoins.
<br/>
<br/>
Principaux risques : obsolescence logicielle, incompatibilités, pertes de performance.
#### 3.8. Interopérabilité
Portée : compatibilité entre différents systèmes d'IA, intégration du système d’IA avec d’autres outils et normes existants, capacité à échanger des informations.
<br/>
<br/>
Principaux risques : verrouillage technologique, incompatibilités de formats<sup><a href="#note7" id="ref7">[7]</a></sup>.
#### 3.9. Respect de l’environnement
Portée : maîtrise de l’empreinte écologique globale associée au développement, au déploiement et à l’exploitation de technologies d’IA, maîtrise de la consommation énergétique, gestion des risques sur l’environnement engendrés par les systèmes d’IA.
<br/>
<br/>
Principaux risques : consommation excessive d’énergie, émissions de CO₂.
#### 3.10. Accessibilité
Portée : accès aux systèmes d'IA, notamment pour les personnes handicapées, gestion des risques d’inégalités engendrés par les systèmes d’IA.
<br/>
<br/>
Principaux risques : exclusion numérique, inégalités d’accès, manque de compatibilité avec les technologies d’assistance.

<a name="note1" id="note1">[1]</a> Notamment [Règlement IA], [ISO/IEC 42001], [Lignes directrices IA de l'UE], [Loi I&L], [NIST AI RMF], [Principes de l'OCDE sur l’IA], [Recommandation IA de l’OCDE], [Recommandations IA de l’ANSSI - 2024], [Recommandations IA de l’ANSSI - 2025], [Recommandations IA de la CNIL], [RGAA], [RGESN], [RGI], et [RGIAF]. [↩](#ref1)
<br/>
<br/>
<a name="note1" id="note1">[2]</a> Manque d’équité par défaut de cadrage du cas d’usage. [↩](#ref2)
<br/>
<br/>
<a name="note1" id="note1">[3]</a> Prises de décisions basées sur des données incomplètes ou non équilibrées. [↩](#ref3)
<br/>
<br/>
<a name="note1" id="note1">[4]</a> Biais dans les données de l'échantillon (si l'ensemble de données d’entrainement n'est pas représentatif de la population), identification et transformation des caractéristiques sensibles (si un groupe défavorisé est présent dans l'échantillon, modifier les pondérations du modèle de manière à modifier le résultat pour ce groupe défavorisé), biais dans la représentation des différentes classes ou catégories de données (ce qui peut entraîner des résultats non représentatifs, inexacts ou injustes), biais dans les corrélations entre les caractéristiques ou les variables utilisées (ce qui peut conduire à des prédictions biaisées). [↩](#ref4)
<br/>
<br/>
<a name="note1" id="note1">[5]</a> Biais introduits à la suite de la conception du modèle, donnant des résultats trompeurs même à partir de données fiables et de qualité, du fait de la construction du modèle (l'architecture du modèle elle-même peut présenter des problèmes inhérents, entraînant des biais tels que le biais de régression, le biais de classification, le biais de clustering, etc. ; il peut y avoir des erreurs de calcul dans les paramètres du modèle, entraînant des modèles sur/sous-ajustés, qui introduisent des biais et du bruit dans les données de sortie) ou de la dérive du modèle (le cas d’usage peut évoluer avec le temps, de sorte que le modèle peut devenir obsolète et nécessiter une re-modélisation et un recyclage au fil du temps, réintroduisant les biais mentionnés ci-dessus à chaque étape). Les impacts potentiels concernent des décisions discriminatoires ou injustes basées sur des caractéristiques personnelles ou des groupes de personnes spécifiques, la partialité ou les préjugés dans les résultats produits par l'algorithme. [↩](#ref5)
<br/>
<br/>
<a name="note1" id="note1">[6]</a> Risque de produire des résultats de sortie discriminatoires ou injustes, qui peuvent avoir un impact négatif sur les utilisateurs ou les parties prenantes concernées, risque de renforcer les stéréotypes ou les préjugés existants à travers les résultats produits par l'IA. [↩](#ref6)
<br/>
<br/>
<a name="note1" id="note1">[7]</a> Comme tout service numérique, ceux qui reposent sur l’IA peuvent ne pas pouvoir interagir avec les autres si les technologies ou les données utilisées ne sont pas cohérentes ou compatibles entre elles. [↩](#ref7)
